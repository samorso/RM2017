---
title: "Multivariate statistics"
subtitle: "Homeworks - correction"
author: "Samuel Orso"
date: "`r Sys.Date()`"
output: 
 prettydoc::html_pretty:
  theme: architect
  highlight: github
  toc: true
  df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction: Multivariate regression
<div id="ex1">
**Exercise 1:** 
```{r,cache=TRUE,eval=TRUE,echo=TRUE,message=FALSE,warnings=FALSE,fig.align='center',fig.height=4,fig.width=6}
# to load the data
# data(women)

# see the dataset
# women

# read explanations about the dataset
# ?women

# 3. Plot
require(ggplot2)
ggplot(women, aes(x = height, y = weight)) + 
  geom_point()

# 5. Fit the regression
fit_lm <- lm(weight ~ height, data = women)

# 6. Summary
summary(fit_lm)

# 7. Change of unit using grid package (install it befor)
# install.package("grid")
require(grid)
new_x <- convertX(unit(173,"cm"),"inches")

# predict using R code, by hand is ok
predict(fit_lm,newdata = data.frame(height = as.numeric(new_x)))
```

4. The regression equation is $$ \text{weight}_i = \beta_0 + \beta_1 \text{height}_i + \varepsilon_i,\quad i = 1,\dots,15.$$

6. The estimated regression equation is $$ \widehat{\text{weight}}_i = `r round(fit_lm$coefficients[1],2)` + `r round(fit_lm$coefficients[2],2)`\; \text{height}_i,\quad i = 1,\dots,15. $$

7. We simply replace the new *Height* in the estimated regression equation to obtain the predicted weight $\widehat{\text{weight}}^{\text{new}}$. The new *Height* needs to be changed to the right unit (from meters to inches). 173 cm is about `r round(as.numeric(new_x),2)` inches (see `R` code). We thus have $$ \widehat{\text{weight}}^{\text{new}} =  `r round(fit_lm$coefficients[1],2)` + `r round(fit_lm$coefficients[2],2)`\; \text{height}^{\text{new}} \approx `r round(round(fit_lm$coefficients[1],2) + round(fit_lm$coefficients[2],2) * round(as.numeric(new_x),2),2)`. $$
</div>

# Descriptive analysis
<div id="ex2">
**Exercise 2** 
```{r,cache=TRUE,eval=TRUE,echo=TRUE,message=FALSE,warnings=FALSE,fig.align='center',fig.height=4,fig.width=6}
# 1.
# set the workind directoy (for me)
setwd("~/Documents/Teaching/RM/")

# to load the data
ski <- read.csv(file = "ski.csv", header = TRUE, sep = ";")

# see the dataset
head(ski)

# 2.
# load the ggplot2 package for plots
require(ggplot2)
require(gridExtra)
plot1 <- qplot(ski$Temperature, x=1, geom = "boxplot") + xlab(NULL) + coord_flip()
plot2 <- qplot(ski$Tickets, x=1, geom = "boxplot") + xlab(NULL) + coord_flip()
grid.arrange(plot1, plot2, ncol=1)

# 3.
ggplot(data = ski, aes(x = Temperature, y = Tickets)) + 
  geom_point() + 
  stat_smooth(method = "lm", col = "red")

# 4.
cor(ski)
cor(ski, method = "spearman")
# Both linear and Spearman's correlation are very close numerically. 
# It does not seem to have outliers. Also on the boxplots and scatterplot, 
# no points seem to be outliers.

# 5. 
fit_lm <- lm(Tickets ~ Temperature, data = ski)
summary(fit_lm)

# 7.
predict(fit_lm, newdata = data.frame(Temperature = -2.5))
```

6. The estimated regression equation is $$ \widehat{\text{Tickets}}_i = `r round(fit_lm$coefficients[1],2)` + `r round(fit_lm$coefficients[2],2)` \;\text{Temperature}_i,\quad i = 1,\dots,`r nrow(ski)`$$
or equivalently you can write $$\hat{y}_i = `r round(fit_lm$coefficients[1],2)` + `r round(fit_lm$coefficients[2],2)` \;x_i,\quad i = 1,\dots,`r nrow(ski)` $$
</div>

# Inference
<div id="ex3">
**Exercise 3:** Will be revealed soon.
```{r,cache=TRUE,eval=TRUE,echo=FALSE,message=FALSE,warnings=FALSE,results='hide'}
# set the workind directoy (for me)
setwd("~/Documents/Teaching/RM/")

# to load the data
hospital <- read.csv(file = "hospital.csv", header = TRUE, sep = ";")

# see the dataset
head(hospital)

# We need the average and the standard deviation
mean(hospital$Days)
sd(hospital$Days)

# We can also compute the quantiles (or use the tables)
qnorm(.975)
qnorm(.95)
qt(.975, df = 43)
qt(.95, df = 43)
```

<!-- 2. We have $\bar{x} = `r (mu <- round(mean(hospital$Days),4))`$ and the quantile $z_{0.975} = `r (z_a <- round(qnorm(.975),4))`$. The 95\% confidence interval is therefore $$ \text{CI}(\bar{x},95\%) = [`r mu` - `r z_a`/\sqrt{44};\; `r mu` + `r z_a`/\sqrt{44}] = [`r round(mu-z_a/sqrt(44),4)`;\;`r round(mu+z_a/sqrt(44),4)`] $$   -->
<!-- 3. The standard deviation is estimated at $\hat{\sigma}\approx `r (sig <- round(sd(hospital$Days),4))`$ and the quantile $t_{43;\;0.975} = `r (t_a <- round(qt(.975,df=43),4))`$. The 95\% confidence interval is therefore $$ \text{CI}(\bar{x},95\%) = [`r mu` - `r t_a`\;`r sig`/\sqrt{44};\; `r mu` + `r t_a`\;`r sig` /\sqrt{44}] = [`r round(mu-t_a*sig/sqrt(44),4)`;\;`r round(mu+t_a*sig/sqrt(44),4)`] $$   -->
<!-- 4. Clearly if the confidence level decreases, there is a lower probability that the confidence interval will contain the true value, it translates into a shorter interval. The quantiles are modifies as follows $z_{0.95} = `r (z_a <- round(qnorm(.95),4))`$ and $t_{43;\;0.95} = `r (t_a <- round(qt(.95,df=43),4))`$. The new confidence intervals are $$ \text{CI}(\bar{x},90\%) = [`r mu` - `r z_a`/\sqrt{44};\; `r mu` + `r z_a`/\sqrt{44}] = [`r round(mu-z_a/sqrt(44),4)`;\;`r round(mu+z_a/sqrt(44),4)`] $$ and  $$ \text{CI}(\bar{x},90\%) = [`r mu` - `r t_a`\;`r sig`/\sqrt{44};\; `r mu` + `r t_a`\;`r sig` /\sqrt{44}] = [`r round(mu-t_a*sig/sqrt(44),4)`;\;`r round(mu+t_a*sig/sqrt(44),4)`] $$   -->

</div>

<div id="ex4">
**Exercise 4:** Will be revealed soon.
```{r,cache=TRUE,eval=TRUE,echo=FALSE,message=FALSE,warnings=FALSE,results='hide',fig.align='center',fig.height=4,fig.width=6,fig.show='hide'}
# 1.
# set the workind directoy (for me)
setwd("~/Documents/Teaching/RM/")

# to load the data
wt <- read.csv(file = "waiting_time.csv", header = TRUE, sep = ";")

# see the dataset
head(wt)

# Descriptive analysis
require(ggplot2)
require(dplyr)

ggplot(wt, aes(y = Time, x = When, group = When, fill = When)) +
  geom_boxplot()

wt %>% group_by(When) %>% summarise(n = length(Time), average = mean(Time), median = median(Time),
                                    max = max(Time), min = min(Time), variance = var(Time))

# 3.
t.test(Time ~ When, mu = 0, alternative = "less", data = wt)

# 4.
# t.test(Time ~ When, mu = 0, alternative = "less", data = wt, paired = TRUE)

# 5. 
wilcox.test(Time ~ When, mu = 0, alternative = "less", data = wt)
```

<!-- 2. It is not clear from the description that we have the exact same clients before/after the change of process. Somehow, it would make more sense to have different clients. Additionnaly,from the descriptive analysis, we can see that the number of clients is inequal. A $t$-test for two different means seems more adequate. Since we are expecting a decrease in waiting time after the change, we should orientate the alternative hypothesis. We make the following hypothesis: $$ H_0:\mu_{\text{Before}} = \mu_{\text{After}} $$ agains $$ H_1: \mu_{\text{Before}} > \mu_{\text{After}} $$ Since there are no indications, we take a default significance level of $\alpha=5\%$.    -->
<!-- 3. Based on the sample and a level of $\alpha=5\%$, we reject $H_0$. We conclude the waiting time decreased significantly after the change of process.     -->
<!-- 4. As already mentionned, the number of unit is different between after/before, so a paired $t$-test is not possible.     -->
<!-- 5. On the boxplot we can see two potential outliers that may influence the mean of the groupe before. This influence may bias the $p$-value towards rejecting the null. The $t$-test and Wilcoxon test do not contradict each, both reject $H_0$ at level of significance of 5\%.  -->
</div>

<div id="ex5">
**Exercise 5:** Will be revealed soon.
```{r,cache=TRUE,eval=TRUE,echo=FALSE,message=FALSE,warnings=FALSE,results='hide',fig.align='center',fig.height=4,fig.width=6,fig.show='hide'}
# 1.
# set the workind directoy (for me)
setwd("~/Documents/Teaching/RM/")

# to load the data
media <- read.csv(file = "medias.csv", header = TRUE, sep = ";")

# see the dataset
head(media)

# Descriptive analysis
require(ggplot2)
require(dplyr)

ggplot(media, aes(y = Impact, x = Media, group = Media, fill = Media)) +
  geom_boxplot()

tbl <- media %>% group_by(Media) %>% summarise(n = length(Impact), average = mean(Impact), variance = var(Impact))
tbl

# 3.
(avg_overall <- mean(media$Impact))
(ss_total <- sum((media$Impact- avg_overall)^2))
(ss_treat <- sum(tbl$n * (tbl$average - avg_overall)^2))
(ss_res <- ss_total - ss_treat)

# 4.
anova_test <- aov(Impact ~ Media, data = media)
summary(anova_test)

# 5. 
pairwise.t.test(media$Impact, media$Media, alternative = "greater", p.adjust.method = "none", pool.sd = FALSE)

# 6.
kruskal.test(Impact ~ Media, data = media)
pairwise.wilcox.test(media$Impact, media$Media, alternative = "greater", p.adjust.method = "none")
```


<!-- 2. There is a clear distinction between `Media` on `Impact`, we have the following order: `Website` > `Television` > `Radio` > `Newspaper`. The distinction between the two last is less evident as `Radio` is more disperse. This order will be uses for pairwise comparisons to answer the question of research. There are no reasons to believe outliers are present in this dataset.    -->
<!-- 3. We have $H_0 :$ all averages are equals agains $H_1:$ at least one difference. We work with a level of significance of $\alpha=0.05$. First we compute the total sum of squares. Then, the treatment sum of squares, from which we can deduce the residual some of squares. Here we can do it by hand, since the dataset is of reasonable size, or by using `R` (see the code). We obtain -->

<!-- Quantity                       | Value -->
<!-- ------------------------------ | -------------- -->
<!-- $\text{df}_1$                  | $3$   -->
<!-- $\text{df}_2$                  | $36$   -->
<!-- $\text{SS}_{\text{Treatment}}$ | `r ss_treat`     -->
<!-- $\text{SS}_{\text{Residuals}}$ | `r ss_res`   -->
<!-- $\text{MS}_{\text{Treatment}}$ | `r (ms_treat <- round(ss_treat/3,4))` -->
<!-- $\text{MS}_{\text{Residuals}}$ | `r (ms_res <- round(ss_res/36,4))` -->
<!-- $t_{\text{obs}}$               | `r (tobs <- round(ms_treat / ms_res,4))`  -->
<!-- $p$-value                      | `r round(1 - pf(tobs,df1=3,df2=36),4)` -->

<!-- We can clearly reject $H_0$ based on this sample and a level of significance of $\alpha=5\%$.     -->
<!-- 4. Same results up to rounding errors.    -->
<!-- 5. The Dunn-Sidak correction is obtained by $$ \alpha_I = 1 - 0.95^{1/6} \approx `r round(1 - 0.95^(1/6),4)`  $$ since there are six different pairs. We cannot conclude on the difference between `Radio` and `Newspaper`, and `Radio` and `Television`, all the other differences are significant with the Dunn-Sidak correction based on this sample. In particular, we can conclude that the `Website` is has the best Impact.   -->
<!-- 6. Since there is no reason to fear influenced from outliers, it seems unnecessary to perform Kruskal-Wallis Wilcoxon tests. Anyway, we perform these tests and found no contradictions. -->
</div>

# Multivariate Regression model
(coming soon)  

# Prediction
(coming soon)
