---
title: "Multivariate statistics"
subtitle: "Reserach Methods I, HES-SO, MScBA"
author: "Samuel Orso"
date: "`r Sys.Date()`"
output: 
 prettydoc::html_pretty:
  theme: architect
  highlight: github
  toc: true
  df_print: kable
  fig_height: 4
  fig_width: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warinings = FALSE, message = FALSE)
```

# Before we get started
### Formalities
Formal information:  

* Lecturer: Samuel Orso (Samuel.Orso@unige.ch)  
* In-class: 20 hours over 5 sessions  
* Evaluation: two group projects (~2+2 students) (25%) and a final exam (75%)  
* Class material on https://cyberlearn.hes-so.ch/   
* The most current version of the course is available on https://samorso.github.io/RM2017/RM.html (if you encounter any bug, you can report it [here](https://github.com/samorso/RM2017/issues)).   

This course serves as an introduction to multivariate statistics with an emphasis on the multivariate regression and its applications. In class, we cover the theory and part of the exercises. The course is self-contained. There exists a large choice of textbooks for self-studying. Here are some options:  

* *Managerial statistics* (2002). S.C. Albright, W.L. Winston, C.J. Zappe.  
* *Managerial statistics* (2012). G. Keller.  
* *Statistics for Business and Economics* (2011). D.R. Anderson, D.J. Sweeney, T.A. Williams.  

### How do we work?
I split the exercises into two categories:  

1. In-class exercises: you will have time during the class to solve these exercises, then I correct it with you. You can find them [here](https://samorso.github.io/RM2017/Inclass.html)  
2. Homework: you have to solve these exercises by your own at home. You receive a detailed solution. In principle I will not give further correction. You can find the exercises [here](https://samorso.github.io/RM2017/hmw.html) and their correction [here](https://samorso.github.io/RM2017/hmw_cor.html)

At the end of each Chapter, I will submit you a quiz. The quiz does not count for your grade but serves as a training for the final exam. You can answer the quiz from your laptop or cell phones. If you intend to use your cell phone, you must install the application `i-nigma`.

### Few more wo`R`ds
This class has been entirely written using `R/RStudio` open source statistical software. The students are strongly encouraged to learn how to use these software by themselves as it will ease their learning process. Also we believe it will be an asset on your CV and for the rest of your careers. Many web sources exist for learning `R`:  

* https://www.rstudio.com/  
* https://www.coursera.org/learn/r-programming/  
* https://www.datacamp.com/  
* http://www.r-exercises.com/  
* and many more ...  

Note that it is **possible** to make the exercises with other software (e.g. Excel), but is is **essential** that you understand the outputs from `R` (as presented in class) for the final examination. For the project you can choose.

To get started, install `R` from https://www.r-project.org/ and `RStudio` https://www.rstudio.com/. Along the document, you will see some windows like the following:
```{r}
# Just an example
1 + 2
```
The gray block is the `R` code that I wrote (here a simple addition of 1 + 2). The white block is the output from `R` (here 3). This course follows the principle of _reproducibility_: you can copy/paste the blocks of code from the slides to your own computer (using `R` of course) to reproduce the same examples used in the slides and familiarize yourself with coding in `R`. When you begin, you will certainly make a lot of mistakes. Doing mistakes and trying to solve it will help you to learn a lot. Some googling is helpful. `R` can also help you if you ask appropriately For example if you want to know more about the function `mean()`, you can write:
```{r}
?mean
```

For less effort (but less gain as well), you can use `R-commander` (http://www.rcommander.com/) in addition to `R/RStudio`. It is a graphical user interface that can prevent you from writing the codes.

`R` has a core software with basic (but very useful) functionalities. It has also tons of external libraries developed by researchers and developers. These libraries are freely available and easy to install. If you want to reproduce the code in the slides, you will need to install few libraries: `ggplot2`, `gridExtra`, `dplyr`. To install a library you can use the following code
```{r,eval=FALSE}
install.packages("ggplot2")
```
The installation is done only once. If you want to use the library, every time you use `R` you need to ask explicitly (but only once per session):
```{r,eval=FALSE}
require(ggplot2)
```

[In-class: exercise 1](https://samorso.github.io/RM2017/Inclass.html#ex1)

# Introduction: Multivariate regression
### Preamble
Multivariate regression aims at modelling the relationship between one **variable of interest** and many (i.e. multivariate) potential **explanatory variables** (or **predictors**). We will use the multivariate regression for two purposes:    

1. Verifying a theory   
2. Predicting  

**Example 1** I suppose that the stopping distance of car depends on the speed of the car. I am interested in the variable *stopping distance* and consider the variable *speed* to potentially explains the phenomena. Here is an illustration based on some data:
```{r, fig.align='center', fig.height=4, fig.width=6, message=FALSE,cache=TRUE}
# load the data
data(cars)

# install package (if not done)
# install.packages("ggplot2")

# load package
require(ggplot2) # for better graphs

# plot the regression
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```
The main line is the **regression line**. The gray area around the regression line is the confidence interval (more on that later). The points are the observations. ''Of course'' the points are not on the line. Two reasons for that might be:    

1. Both the stopping distance and the speed are random variables;  
2. The relation between the variable of interest and the explanatory variable is not **linear**.  
The regression line is interpreted as follows: the faster the car drives, the more distance the car needs to stop, which seems to make sense. What else could we consider to explain the stopping distance?

[In-class: exercise 2](https://samorso.github.io/RM2017/Inclass.html#ex2)

### From mathematics to statistics
You certainly remember the line equation from your math courses: $$ y = ax + b,$$ where $b$ is the intercept and $a$ the slope. Of course it can be extended to more than one $x$'s. This is somehow the ''ideal'' model that we believe to hold when we work with regression. If we could observe this *idealized* model, we should observe something like:

```{r, fig.align='center', fig.height=4, fig.width=6, message=FALSE,cache=TRUE}
# Load and install package (if not done)
# install.packages("ggplot2")
# require(ggplot2)

# Create a data frame (necessary for ggplot2)
df <- data.frame(
  speed = seq(from = min(cars$speed), to = max(cars$speed)), # sequence of integers on the range of "speed"
  dist = -17.5791 + 3.9324 * seq(from = min(cars$speed), to = max(cars$speed)) # equation of the line
)

# Plot the data frame
ggplot(data=df, aes(x=speed, y=dist, group=1)) +
  geom_point()
```


When it comes to sample observations, all becomes random. There exist many sources of variability: rounding, error in measurement, ... Consequently we observe that instead:

```{r, fig.align='center', fig.height=4, fig.width=6, message=FALSE,cache=TRUE}
# Load and install package (if not done)
# install.packages("ggplot2")
# require(ggplot2)

# Load the data (if not done)
# data(cars)

# Plot the data
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point()
```

The general idea of regression is to find back the line equation from the observations. Two reasons for that:   

1. Check whether $x$ has a linear impact on $y$;     
2. Predict future values for $y$.   

### Formalization
The multivariate regression can be formalized as follows. Suppose that you are interested in the variable $y$ and have $p$ potential predictors $x$. You have obtained a sample of size $n$. The **regression equation** is given by $$ y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_p x_{pi} + \varepsilon_i,\quad i = 1,\dots,n, $$ or more compactly, $$ y_i = \beta_0 + \sum_{j=1}^px_{ij}\beta_j + \varepsilon_i,\quad i = 1,\dots,n. $$
There are two unknown quantities:  

1. the $\beta$'s are the **regression parameters**. Note that $\beta_0$ has a specific name: the *intercept*.  
2. $\varepsilon$ represent the random errors of the regression line. **They are not observable**. However, it is assumed to follows independently and identically a normal distribution centered on 0 and of variance $\sigma^2_\varepsilon$, $\varepsilon_i\sim\mathcal{N}(0,\sigma^2_\varepsilon)$.

These two unknown quantities are assumed to be fixed constant. We can **estimate** them based on the observations, i.e. we find numerical values for them. We denote the **estimators** with a ''hat'': $\hat{\beta}$ is the estimator of $\beta$, $\hat{\sigma}^2_\varepsilon$ is the estimator of $\sigma^2_\varepsilon$. The $\hat{\beta}$'s are called the **regression coefficients**.

**Example 1** We can write the regression equation as $$ \text{dist}_i = \beta_0 + \beta_1\text{speed}_i + \varepsilon_i,\quad i = 1,\dots,50. $$ To obtain the estimators, we run the following code:
```{r,cache=TRUE}
# Load the data (if not done)
# data(cars)

# Estimate the Linear Model
estim_lm <- lm(dist ~ speed, data=cars)

# Summary of the estimation
summary(estim_lm)
```
The summary has a lot of useful information. For the moment we are only interested in the estimators. We have:  

* The intercept: $\hat{\beta}_0 =$ `r round(estim_lm$coefficients[1],4)`;  
* The coefficient of the variable speed: $\hat{\beta}_1$ = `r round(estim_lm$coefficients[2],4)`;  
* The square root of the estimator of the variance: $\sqrt{\hat{\sigma}^2_\varepsilon} =$ `r round(sqrt(sum((estim_lm$residuals - mean(estim_lm$residuals))^2)/estim_lm$df.residual),2)`.  

Once we obtain the **estimators**, we can write the **estimated regression equation**: $$ \hat{y}_i = \hat{\beta}_0 + \sum_{j=1}^px_{ij}\hat{\beta}_j,\quad i = 1,\dots,n. $$ From this estimated regression equation, we can also obtain values for the *unobserved* errors $\varepsilon$. It is simply defined as $$ \hat{\varepsilon}_i = y_i - \hat{y}_i,\quad i=1,\dots,n. $$ The $\hat{\varepsilon}$'s are called the **residuals**. They are useful in part because we can approximately verify that $\varepsilon$ follows a normal distribution. I insist that this verification is approximate.

**Example 1** The estimated regression equation is given by $$ \widehat{\text{dist}}_i = `r round(estim_lm$coefficients[1],4)` + `r round(estim_lm$coefficients[2],4)`\;\text{speed}_i, \quad i = 1,\dots,50. $$
```{r, fig.align='center', fig.height=4, fig.width=6, message=FALSE,cache=TRUE}
# Load and install package (if not done)
# install.packages("ggplot2")
# require(ggplot2)

# Load the data (if not done)
# data(cars)

# Obtain residuals from the estimated linear regression
res <- estim_lm$residuals
hat_sigma_eps <- sqrt(sum((estim_lm$residuals - mean(estim_lm$residuals))^2)/estim_lm$df.residual)

# Overlay histogram and normal density
qplot(res, geom = 'blank') +   
  stat_function(fun = dnorm, aes(colour = 'Normal'), args = list(sd = hat_sigma_eps)) +          
  geom_histogram(aes(y = ..density..), alpha = 0.4, binwidth = 3) +                        
  scale_colour_manual(name = 'Density', values = 'blue') + 
  ggtitle("Residuals plot") + 
  xlab(expression(hat(epsilon))) +
  ylab("Density") + 
  annotate('text',x=-26,y=0.025,label="X~follows~N~(0~~hat(sigma)[epsilon]^2)",parse=TRUE,size=3,col='blue')
```

Also we can use the estimated regression to obtain predictions about the variable of interest. Suppose that we observe new values for $x$, denoted $x^{\text{new}}$, then we have **estimated prediction** of a new $y$ $$ \hat{y}^{\text{new}} = \hat{\beta}_0 + \sum_{j=1}^px^{\text{new}}_{ij}\hat{\beta}_j$$

**Example 1** Suppose we observe a new car with speed measured at 15. The predicted distance is $$ \widehat{\text{dist}}^\text{new} = `r round(estim_lm$coefficients[1],4)` + `r round(estim_lm$coefficients[2],4)`\cdot 15 \approx `r round(estim_lm$coefficients[1] + estim_lm$coefficients[2] * 15,4)` $$

### Case study: how much is the car worth?
Let us introduce a case study that will follow us during the class. For this data set, a representative sample of over eight hundred 2005 GM cars were selected. The data set contains the following variables:

* *Price*: suggested retail price of the used 2005 GM car in excellent condition. The condition of a car can greatly affect price. All cars in this data set were less than one year old when priced and considered to be in excellent condition.
* *Mileage*: number of miles the car has been driven.   
* *Make*: manufacturer of the car such as Saturn, Pontiac, and Chevrolet.   
* *Model*: specific models for each car manufacturer such as Ion, Vibe, Cavalier.   
* *Trim* (of car): specific type of car model such as SE Sedan 4D, Quad Coupe 2D          
* *Type*: body type such as sedan, coupe, etc.      
* *Cylinder*: number of cylinders in the engine        
* *Liter*: a more specific measure of engine size     
* *Doors*: number of doors           
* *Cruise*: indicator variable representing whether the car has cruise control (1 = cruise)
* *Sound*: indicator variable representing whether the car has upgraded speakers (1 = upgraded)
* *Leather*: indicator variable representing whether the car has leather seats (1 = leather)

*Price* is the variable of interest. There are 11 potential predictors.

```{r,eval=TRUE,cache=TRUE}
# Set the working directory of R to your own directory 
setwd("~/Documents/Teaching/RM/") # in my case, in your computer you should indicate your own path
getwd() # to check the current directory

# load the dataset
data <- read.csv(file="kuiper.csv",sep=",")

# Check the first six lines
head(data)
```

[In-class: exercise 3](https://samorso.github.io/RM2017/Inclass.html#ex3)    
[Homework: exercise 1](https://samorso.github.io/RM2017/hmw.html#ex1)

# Descriptive analysis
The aims of descriptive statistics are:  

* To focus on some characteristics of the sample  
* To represent the data in a readable way  
* To summarize the sample information  

These summaries can be numerical or graphical, univariate or multivariate.
For example the *5-number-summary* provides numerical summary of an univariate variable:   

* Minimum or quantile at 0%: the smallest data, 0% of the data are
smaller  
* 1st quartile or quantile at 25%: a quarter of the data (25%) are
smaller, the 75% of the data are greater.  
* Median or quantile at 50%: half of the data (50%) are smaller, half
are greater.   
* 3rd quartile or quantile at 75%: a quarter of the data (25%) are
greater, the 75% of the data are smaller.  
* Maximum or quantile at 100%: The greatest data.   

### Application to regression: variable of interest
The variable of interest is assumed to follow approximately a normal distribution in the regression context. We should observe (approximately) this behavior with *boxplot* or *histogram*. What is this behavior?

**Example 2:** Suppose we observe a sample of size 100 from a normal random variable $X\sim\mathcal{N}(0,1)$.
```{r,fig.align='center', fig.height=6, fig.width=6, message=FALSE,cache=TRUE}
# load a package
require(gridExtra)

# Generate normal random variates
set.seed(123)
x <- rnorm(100)

# Plot the histogram and boxplot
plot1 <- qplot(x, geom="histogram",bins=15)
plot2 <- qplot(x, x=1, geom="boxplot") + xlab(NULL) + coord_flip()
grid.arrange(plot1, plot2, ncol=1)
```


**CS:** Here is the 5-number-summary and the histogram/boxplot of the price.
```{r,fig.align='center', fig.height=6, fig.width=6, message=FALSE,cache=TRUE}
# 5-number-summary
summary(data$Price)

# Plot the histogram and boxplot
plot1 <- qplot(data$Price, geom="histogram")
plot2 <- qplot(data$Price, x=1, geom="boxplot") + xlab(NULL) + coord_flip()
grid.arrange(plot1, plot2, ncol=1)
```

Here the price is right skewed. Let see what happens if we log-transform the variable.

```{r,fig.align='center', fig.height=6, fig.width=6, message=FALSE,cache=TRUE}
# load a package
require(gridExtra)

# Plot the histogram and boxplot
plot1 <- qplot(log(data$Price), geom="histogram")
plot2 <- qplot(log(data$Price), x=1, geom="boxplot") + xlab(NULL) + coord_flip()
grid.arrange(plot1, plot2, ncol=1)
```

### Application to regression: continuous predictors
Many assumptions on the predictors can be intuitively check via a bivariate graph, the **scatterplot**:   

* Are the predictors (more or less) independent between them? We should observe a *widespread cloud* on a scatterplot.   
* Are the predictors **linearly dependent** with the variable of interest? On a scatterplot, we should be able to imagine a line going through the points.   
* The distance between the points and this imaginary line should be more or less constant, or saying it differently error should be **homoscedastic** (meaing *same variability*), otherwise we might have a problem of **heteroskedasticity**. In which case it might be a good option to make a transformation.   
* Are there outliers on the data?   
We can also use the **linear correlation** to measure the strength of the linear relationship. The correlation varies between -1 to 1. 1 represent a strong positive (down-left to upright) linear relationship. -1 represents a strong negative linear relationship. 0 represents no linear relationship.

**CS:** Let see the scatterplot of *Mileage* against *Liter*.
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
qplot(data$Mileage,data$Liter)
```
It seems hard to imagine a straight line. The correlation = `r round(cor(data$Mileage,data$Liter),4)`. Why the graph looks like that?

**Example 3:** We receive some characteristics about cars. We look at the scatterplot of the variables *miles per gallon* versus *weights*.
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
qplot(mpg,wt,data=mtcars)
```
The correlation is `r round(cor(mtcars$mpg,mtcars$wt),4)` (you can use the function `cor(x,y)`).

**Example 4:** Suppose that we observe the following:
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
# Generate random values
set.seed(1234)
x <- rnorm(1000)
y <- x * x + rnorm(1000)

# Plot
qplot(x,y)
```
The correlation is `r round(cor(x,y),4)`. Evidently both $x$ and $y$ have a relationship, but it is not linear, it is squared, or more generally we say **curvilinear**.

**Example 5:** Suppose that we observe the following:
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
# Generate random values
set.seed(1234)
x <- rnorm(1000)
y <- x + rnorm(1000)

# We add randomly some outliers
y[sample(1000,10)] <- rnorm(10,mean=15)

# Plot  
qplot(x,y)
```
The correlation is `r round(cor(x,y),4)`. The **Spearman's rank correlation** is  `r round(cor(x,y,method="spearman"),4)`. We say that Spearman's rank correlation is **robust** to outliers.

[In-class: exercise 4](https://samorso.github.io/RM2017/Inclass.html#ex4)

### Application to regression: discrete predictors
If predictors are discrete (factors, few values, ...), scatterplot and 5-number-summary are not suitable. It may be more convenient to use **barplot** and **table**.

**CS:** Let's look at the predictor Cylinder
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
# Table
table(data$Cylinder)

# Barplot
qplot(data$Cylinder, geom="bar")
```

Discrete predictors can be combined with a continuous predictor/variable of interest with multiple boxplots.

**CS:** Let's look at the relation between Cylinder and Price.
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
# Multiple boxplots
ggplot(data=data, aes(x=Cylinder, y=Price, group=Cylinder, fill=Cylinder)) +
  geom_boxplot()
```

[In-class: exercise 5](https://samorso.github.io/RM2017/Inclass.html#ex5)   
[Homework: exercise 2](https://samorso.github.io/RM2017/hmw.html#ex2)   

# Inference
While descriptive statistics describe characteristics of a **sample** (location, dispersion, graphs, ...), inferential statistics provide information on the underlying **population**. This task is much more useful than a simple description but also much more difficult.

## Estimation by intervals
The goal of **confidence intervals** is to quantify the uncertainty around the **point estimates** (single values) to be representative of the population. It gives a range such that the true value of the parameter in the population is contained in the interval with a given probability. The confidence probability is set by the user and is called the **confidence level**.

**Example 2:** Suppose we observe a sample of size 100 from a normal random variable $X\sim\mathcal{N}(0,1)$. We are interested in the mean. The population mean is 0 here, but generally it is unknown. The sample mean, the point estimate, is `r set.seed(123); round(mean(rnorm(100)),4)`. A confidence interval at 95% is given by $[`r set.seed(123); round(mean(rnorm(100))-1.96/10,4)`,`r set.seed(123); round(mean(rnorm(100))+1.96/10,4)`]$. 

In Example 2, we say that there is 95% probability that this interval contains the population mean (and not the converse). It is important to note that there is still 5% chance that this interval does not contain the population mean.

More generally, a 1-$\alpha$% confidence interval for a sample mean is given by the general formula $$ \Big[\bar{x} - z_{1-\alpha/2}\sqrt{\frac{\sigma^2}{n}};\;\bar{x} + z_{1-\alpha/2}\sqrt{\frac{\sigma^2}{n}}\Big], $$ where $z_\alpha$ is the $\alpha$-quantile of a standard Normal distribution (recall your probability classes). The user chooses the level $\alpha$. Generally the default value is $\alpha=0.05$, but it may vary depending the context. This formula assumes that the variance $\sigma^2$ is a known value. In most cases the **variance is unknown** and is estimated, which leads to the following modification $$ \Big[\bar{x} - t_{n-1;\;1-\alpha/2}\sqrt{\frac{\hat{\sigma}^2}{n}};\;\bar{x} + t_{n-1;\;1-\alpha/2}\sqrt{\frac{\hat{\sigma}^2}{n}}\Big], $$ where $t_{\text{df};\;\alpha}$ is the $\alpha$-quantile of the student distribution with $\text{df}$ degrees of freedom ($n$ is the sample size).

#### Application to regression: confidence interval on a coefficient
Regression coefficients can be treated as average, we can use the same formulas. The only difference is that the degrees of freedom is not $n-1$, but $n-p-1$ where $p$ is the number of regressors.

**Example 1:** Recall that we try to explain the distance a car takes to stop with the speed of the car. The fitted linear regression is given by
```{r}
summary(estim_lm)
```
The column `Estimate` gives the point estimates: $\hat{\beta}_0,\hat{\beta}_1$. The next column `Std. Error` gives an estimate of the standard errors: $\sqrt{\hat{\sigma}^2_0/\text{df}},\sqrt{\hat{\sigma}^2_1/\text{df}}$. **Be careful, standard errors is not standard deviation, but standard deviation divided by the square root of the degrees of freedom!!** To construct a 95% confidence interval ($\alpha$=0.05), we need the sample size, here $n=50$, and the quantile of the student distribution, here $t_{n-2;\;1-\alpha/2}=t_{48;0.975}=`r round(qt(.975,df=48),4)`$. For the intercept we have $$ CI(\hat{\beta}_0,95\%) = \Big[-17.5791 - 2.0106\cdot6.7584;\;-17.5791 + 2.0106\cdot6.7584\Big] = \Big[-31.1675;\;-3.9907\Big].$$ And for the coefficient of speed, we have $$CI(\hat{\beta}_1,95\%) = \Big[3.9324 - 2.0106\cdot0.4155;\;3.9324 + 2.0106\cdot 0.4155\Big] = \Big[3.0970;\;4.7678\Big].$$

[Homework: exercise 3](https://samorso.github.io/RM2017/hmw.html#ex3)   

## Hypothesis testing
The objective of hypothesis testing is to answer a question about the population under study. It is a major element of statistical inference. Hypothesis testing is constructed in three parts:   

1. What hypothesis am I testing?    
2. What is my decision rule?   
3. Based on the sample, what is my decision?   

It is important to note that the two first parts are chosen first and independently of the sample (i.e. before looking at the data). The three parts are standard for every test.

### Hypothesis
A hypothesis testing is always constructed with a **null-hypothesis**, denoted $H_0$, and the **alternative hypothesis**, denoted $H_1$. $H_0$ is the default hypothesis, one chooses this hypothesis by default. Sometimes we want to prove $H_1$, that is we want to **reject $H_0$**, or sometimes we want to disprove $H_1$, i.e. we want to fail rejecting $H_0$.

**Example 1:** We want to test whether the predictor *Speed* has no linear impact on the *Distance* (null-hypothesis) against *Speed* has an impact (alternative hypothesis). It is formalized as follows: $$ H_0\;:\;\beta_1=0, \\ H_1\;:\;\beta_1\neq0. $$

For Example 1, if we reject $H_0$, it means that we believe the population $\beta_1$ is not 0, that is *Speed* has a linear impact on *Distance*. Of course the sample coefficient, $\hat{\beta}_1$, will never be exactly 0 even if the population coefficient is, this is why we need statistical inference!

### Decision rule
Taking the decision to reject or to not reject $H_0$ is risky. Before to make any decision, we need to agree on what risk we are ready to take. Here is summary of the existing risks:

Decision / Population | $H_0$ is true     | $H_0$ is false
--------------------- | ----------------- | -----------------
Reject $H_0$          | Type I error      | Correct inference
Fail to reject $H_0$  | Correct inference | Type II error

or, in a trial:

Decision / Reality | Innocent        | Guilty
------------------ | --------------- | -----------------
Guilty             | Serious mistake | Correct
Innocent           | Correct         | Mistake

We cannot eliminate these sources of error completely at the same time:   

* A rule completely eliminating type I error would always conclude in favor of $H_0$ even when it is wrong ("every defendant is innocent")     
* A rule completely eliminating type II error would always conclude in favor of $H_1$ even when it is wrong ("every defendant is guilty")    

Consequently one has to choose the risk he/she is ready to take by choosing a **significant level**, denoted $\alpha$. The significance level $\alpha$ control the error of type I (the most serious one). It is common to choose $\alpha=5\%$. It means ''I am ready to be wrong 5\% of the time when rejecting $H_0$''.

The $p$-value is a number obtained on the data. It determines the decision. The **decision rule** is the following:     

* If the $p$-value is smaller than the significance level $\alpha$, we reject $H_0$. It means that we gathered enough evidence against $H_0$ and accept $H_1$. We call the test **significant**.   
* If the $p$-value is greater than $\alpha$, the test is **unsignificant**, no conclusion can be reached based on the sample at hand.   

More specifically, the $p$-value is the probability under $H_0$ of observing a **test statistic** more in favor of $H_1$ than the test statistic indeed observed. Generally $T$ denotes the test statistic (a random variable), and $t_\text{obs}$ the observed statistic on the sample. 

Instead of comparing the $p$-value with the significance level, it is equivalent to compare $t_{\text{obs}}$ with a quantile of the distribution of $T$ depending on $\alpha$, denoted $T(\alpha)$. This quantile $T(\alpha)$ determines a **rejecting region**, which leads to the following **decision rule** (totally equivalent to the previous one):   

* If $t_{\text{obs}}$ is included in the rejecting region, we reject $H_0$. It means that we gathered enough evidence against $H_0$ and accept $H_1$. We call the test **significant**.   
* If $t_{\text{obs}}$ is not included in the rejection region, the test is **unsignificant**, no conclusion can be reached based on the sample at hand. 

We will see how to compute $p$-values and rejection region in examples.

### Test on a single mean
We want to **test an average** with **known variance**. We have a sample of $n$ observations from the population with average $\mu$ (unknown) and variance $\sigma^2$ (known). The aim is to test if the population average is equal to an hypothetical value $\mu_0$ (chosen by the tester). The significance level is $\alpha$.   

 Interest          |  Value                                                  
------------------ | -----------------------------------------------------------
Test statistic     | $T = \frac{\bar{X}-\mu_0}{\sqrt{\sigma^2 /n}}$
$H_0$              | $\mu = \mu_0$
$H_1$ (two-sided)  | $\mu\neq\mu_0$
$p$-value          | $\mathbb{P}_{H_0}\big(\{T>\lvert t_{\text{obs}\rvert}\}\cup\{T<-\lvert t_{\text{obs}}\rvert\}\big) = 2-2\Phi(\lvert t_{\text{obs}}\rvert)$
$T(\alpha)$        | $z_{1-\alpha/2}$
Rejection region   | $(-\infty,-z_{1- \alpha/2}]\cup[z_{1- \alpha/2},+\infty)$


where $z_\alpha$ denotes the $\alpha$-quantile of standard normal distribution and $\Phi(z)=\mathbb{P}(Z<z)$ is the cumulative distribution function of the standard normal distribution. Note the similitude between confidence interval and rejection region! 

Here is a small illustration of the concepts. Suppose we test with a level $\alpha=5\%$ whether $\mu=\mu_0=0$. We know that $z_{1-\alpha/2}=1.96$ (check the table of the normal law if you are unsure). Suppose we compute the observed statistic and obtain $t_{\text{obs}} = -2.1$. We can then illustrate the $p$-value and rejection region:
```{r,fig.align='center',fig.width=6,fig.height=8,cache=TRUE}
# Values
alpha <- 0.05
tobs <- -2.1
p_val <- 2 - 2 * pnorm(abs(tobs))
p_val
z_alpha <- qnorm(1 - alpha/2)
z_alpha

# Plot for p-value
plot1 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(-3.5,-abs(tobs)), geom = "area", fill="red", alpha=0.5) + 
        stat_function(fun = dnorm, xlim = c(abs(tobs),3.5), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("p-value for two-sided test",) +
        annotate('text',x=-2.5,y=.2,label="p-value~is~0.0357",parse=TRUE,size=3,col='red') +
        geom_segment(data=data.frame(x=c(-2.5,-2.45),y=c(.19,.19),xe=c(-2.5,1.9),ye=c(.03,.03)),
                     mapping=aes(x=x,y=y,xend=xe,yend=ye), 
                     arrow=arrow(), size=1, color="red") + 
        annotate('text',x=-abs(tobs),y=-.02,label="-~'|'~t[obs]~'|'",parse=TRUE,size=3,col='red') + 
        annotate('text',x=abs(tobs),y=-.02,label="'|'~t[obs]~'|'",parse=TRUE,size=3,col='red')

# Plot for rejection region
plot2 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(-3.5,-z_alpha), geom = "area", fill="red", alpha=0.5) + 
        stat_function(fun = dnorm, xlim = c(z_alpha,3.5), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("rejection region for two-sided test",) +
        geom_vline(xintercept = tobs, col="red") +
        annotate('text',x=tobs-.15,y=.2,label="t[obs]",parse=TRUE,size=3,col='red')
        
# Both graphs
grid.arrange(plot1, plot2)
```
The $p$-value is $<5\%$, so we would reject $H_0$ according to the decision rule. The $t_{\text{obs}}$ is included in the rejection region, so we would also reject $H_0$ according to the decision rule. Both the $p$-value and the rejection region leads to the same conclusion because both are exactly the same decision rule, they do not contradict each other!

This test has the possibility to be oriented: it can be more precise on the conclusion.

 Interest          |  Value                                                  
------------------ | -----------------------------------------------------------
$H_0$              | $\mu = \mu_0$
$H_1$ (oriented)   | $\mu<\mu_0$ or $\mu>\mu_0$
$p$-value          | $\mathbb{P}_{H_0}\big(T< t_{\text{obs}}\big) = \Phi( t_{\text{obs}})$ or $\mathbb{P}_{H_0}\big(T> t_{\text{obs}}\big) = 1-\Phi( t_{\text{obs}})$
$T(\alpha)$        | $z_{1-\alpha}$
Rejection region   | $(-\infty,-z_{1- \alpha}]$ or $[z_{1- \alpha},+\infty)$


Let see how the previous illustration is modified if we make the alternative hypothesis that $H_1:\mu<\mu_0$ (here $\mu_0=0$).
```{r,fig.align='center',fig.width=6,fig.height=8,cache=TRUE}
# p-value
p_val_1 <- pnorm(tobs)
p_val_1
z_alpha_ <- qnorm(1 - alpha)
z_alpha_

# Plot for p-value
plot1 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(-3.5,tobs), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("p-value for oriented test",) +
        annotate('text',x=-2.5,y=.2,label="p-value~is~0.0179",parse=TRUE,size=3,col='red') +
        geom_segment(data=data.frame(x=-2.5,y=.19,xe=-2.5,ye=.03),
                     mapping=aes(x=x,y=y,xend=xe,yend=ye), 
                     arrow=arrow(), size=1, color="red") + 
        annotate('text',x=tobs,y=-.02,label="t[obs]",parse=TRUE,size=3,col='red') 

# Plot for rejection region
plot2 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(-3.5,-z_alpha_), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("rejection region for oriented test",) +
        geom_vline(xintercept = tobs, col="red") +
        annotate('text',x=tobs-.15,y=.2,label="t[obs]",parse=TRUE,size=3,col='red')
        
# Both graphs
grid.arrange(plot1, plot2)
```

Clearly we are more precise ($p$-value smaller) in this direction. Note that the $p$-value of the two-sided test is divided by 2 to obtain the $p$-value of this oriented test. What happens if the alternative hypothesis is $H_1:\mu>\mu_0$? Let see
```{r,fig.align='center',fig.width=6,fig.height=8,cache=TRUE}
# p-value
p_val_2 <- 1-pnorm(tobs)
p_val_2

# Plot for p-value
plot1 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(tobs,3.5), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("p-value for oriented test",) +
        annotate('text',x=-2.5,y=.2,label="p-value~is~0.9821",parse=TRUE,size=3,col='red') +
        geom_segment(data=data.frame(x=-2.5,y=.19,xe=-2,ye=.1),
                     mapping=aes(x=x,y=y,xend=xe,yend=ye), 
                     arrow=arrow(), size=1, color="red") + 
        annotate('text',x=tobs,y=-.02,label="t[obs]",parse=TRUE,size=3,col='red') 

# Plot for rejection region
plot2 <- ggplot(data.frame(x = c(-3.5,3.5)), aes(x)) + 
        stat_function(fun = dnorm, size=1.5) + 
        stat_function(fun = dnorm, xlim = c(z_alpha_,3.5), geom = "area", fill="red", alpha=0.5) + 
        xlab("T") + ylab("density") + ggtitle("rejection region for oriented test",) +
        geom_vline(xintercept = tobs, col="red") +
        annotate('text',x=tobs-.15,y=.2,label="t[obs]",parse=TRUE,size=3,col='red')
        
# Both graphs
grid.arrange(plot1, plot2)
```
With this alternative, we could not reject $H_0$. Note that this $p$-value is obtained by taking 1 minus the $p$-value of the other oriented test. Of course, it is important that the alternative is carefully chosen before looking at the result. First, you choose the decision rule (hypothesis and $\alpha$) and only then compute the $p$-value or the rejection region.  


**CS:** We read that a used car should cost 18'000\$. We want to test this assertion with case study data. We are willing to take 5% chance to be wrong when rejecting this hypothesis. The variance is assumed to be $\sigma^2=5\times 10^5$.

 Interest          |  Value                                                  
------------------ | -----------------------------------------------------------
Observed statistic | $t_{\text{obs}} = \frac{21343- 18000}{\sqrt{5\times 10^5/804}}\approx 134.05$
$H_0$              | $\mu = 18000$
$H_1$              | $\mu\neq18000$
$p$-value          | $2-2\Phi(134.05)\approx 0$
$T(\alpha)$        | $z_{0.975}=1.96$
Rejection region   | $(-\infty,-1.96]\cup[1.96,+\infty)$

Since $p$-value<0.05, or equivalently $t_{\text{obs}}$ is included in the rejection region, we reject $H_0$ based on this data and a level of $\alpha=0.05$.

Note that you can obtain the value for the $p$-value with the following code
```{r}
2 - 2 * pnorm(134.05)
```

If the **variance is unknown**, we need an estimate, i.e. $\hat{\sigma}^2$. The test is modified as follows (similar for oriented test)

Interest           |  Value                                                  
------------------ | -----------------------------------------------------------
Test statistic     | $T = \frac{\bar{X}-\mu_0}{\sqrt{\hat{\sigma}^2 /n}}$
$p$-value          | $\mathbb{P}_{H_0}\big(\{T>\lvert t_{\text{obs}\rvert}\}\cup\{T<-\lvert t_{\text{obs}}\rvert\}\big) = 2-2F_{n-1}(\lvert t_{\text{obs}}\rvert)$
$T(\alpha)$        | $t_{n-1;\;1-\alpha/2}$
Rejection region   | $(-\infty,-t_{n-1;\;1-\alpha/2}]\cup[t_{n-1;\;1-\alpha/2},+\infty)$

Note that $F_{\text{df}}$ denotes the cumulative distribution function of the student-$t$ distribution with $\text{df}$ degrees of freedom (here $n-1$). As seen previously, $t_{\text{df};\;\alpha}$ is the $\alpha$-quantile of a student-$t$ distribution. This is the famous **$t$-test**!


**CS:** We make the same test (with the testing hypothesis $H_0: \mu=18000$ and $H_1:\mu\neq18000$), but this time the variance is estimated. We obtain $\hat{\sigma}^2 \approx 9.8\times10^7$.
 
 Interest          |  Value                                                  
------------------ | -----------------------------------------------------------
Observed statistic | $t_{\text{obs}} \approx \frac{21343- 18000}{\sqrt{9.8\times 10^7/804}}= 9.5753$
$p$-value          | $2-2F_{803}(9.5753)\approx 0$
$T(\alpha)$        | $t_{803;\;0.975}\approx 1.96$
Rejection region   | $(-\infty,-1.96]\cup[1.96,+\infty)$

Note that you can compute the $p$-value using `R`
```{r}
2 - 2 * pt(9.5753, df = 803)
```

Or you can directly use the `t.test()` function
```{r}
t.test(x = data$Price, mu = 18000, alternative = "two.sided")
```
What is the conclusion of the test?

If you suspect your data has outliers, the mean might be influenced (see previous section), and it might be a good idea to consider a robust alternative. **Wilcoxon's signed-rank test** is an alternative choice to the $t$-test. The testing hypothesis are the same, but values of interest are too complicated to obtain (for the scope of this class).

From previous example, we can perform a Wilcoxon test by using the `wilcox.test()` function as follows
```{r}
wilcox.test(x = data$Price, mu = 18000, alternative = "two.sided")
```

#### Application to regression: test on a single coefficient
Testing for a single regression coefficient uses the same framework as testing a single mean. The difference is in the degrees of freedom, previously we had $\text{df}=n-1$, now in regression we have $\text{df}=n-p-1$ where $p$ is the total number of regressors.

**Example 1:** We want to test whether the predictor *Speed* has a null coefficient at a level of $\alpha=5\%$. (What are the hypothesis?)

 Interest          |  Value                                                  
------------------ | -----------------------------------------------------------
Observed statistic | $t_{\text{obs}} \approx \frac{3.9324}{0.4155}= 9.4643$
$p$-value          | $2-2F_{48}(9.4643)\approx 1.49\times10^{-12}$
$T(\alpha)$        | $t_{48;\;0.975}\approx 2.0106$
Rejection region   | $(-\infty,-2.0106]\cup[2.0106,+\infty)$

Clearly, there is enough evidence to reject $H_0$, the test is significant, we conclude that $\beta_1\neq0$ with a significant level of 5\%.
What would you have said if you were provided only the confidence interval?

Compare the values with the `R` output:
```{r}
summary(estim_lm)
```

[In-class: exercise 6](https://samorso.github.io/RM2017/Inclass.html#ex6)   

### Test on two averages
We see two different situations of interest: a test on the averages of two populations and a test on the average of the same population but in two different situations.

#### Two populations
We want to make a test on the difference in average, denoted $\delta$, between two population with different variances, i.e. $\sigma^2_1\neq\sigma^2_2$. There are two samples size are $n_1$ and $n_2$, the sample averages are $\bar{x}_1$ and $\bar{x}_2$, and the sample standard deviations are $\hat{\sigma}_1$ and $\hat{\sigma}_2$.

 Interest          |  Value                                                  
------------------ | ----------------------------------------------------------
Test statistic     | $T = \frac{(\bar{x}_1-\bar{x}_2)-\delta}{\sqrt{\hat{\sigma}^2_1/n_1 + \hat{\sigma}^2_2/n_2}}$
$H_0$              | $\mu_1-\mu_2 = \delta$
$H_1$ (two-sided)  | $\mu_1-\mu_2 \neq \delta$ (also possible to orientate the test)
$p$-value          | $\mathbb{P}_{H_0}\big(\{T>\lvert t_{\text{obs}\rvert}\}\cup\{T<-\lvert t_{\text{obs}}\rvert\}\big) = 2-2F_{\text{df}}(\lvert t_{\text{obs}}\rvert)$
$T(\alpha)$        | $t_{\text{df};\;1-\alpha/2}$
Rejection region   | $(-\infty,-t_{\text{df};\;1-\alpha/2}]\cup[t_{\text{df};\;1-\alpha/2},+\infty)$

Here the degrees of freedom is a bit more complicated, it is obtained by the following formula: $$ \text{df} = \frac{(\hat{\sigma}^2_1/n_1 + \hat{\sigma}^2_2/n_2)^2}{(\hat{\sigma}^2_1/n_1)^2/(n_1-1) + (\hat{\sigma}^2_2/n_2)^2/(n_2-1)} $$
This is the Welch $t$-test.

**CS:** We want to test whether there is a price difference between cars with 4 and 6 cylinders at a significance level of $\alpha=10\%$. We are testing the hypothesis $$ H_0:\mu_1-\mu_2=0 $$ against $$ H_1:\mu_1-\mu_2\neq0 $$
Here is a simple `R` code to obtain the necessary values on the sample
```{r,warning=FALSE,message=FALSE}
# load the dplyr package (first it needs to be installed)
require(dplyr)

# Get the seeked values 
data %>% group_by(Cylinder) %>% summarize(averages=mean(Price), variances=var(Price), n=length(Price))
```

From these values, we obtain the observed test statistic $t_{\text{obs}} \approx `r round((17862.56-20081.40)/sqrt(61324307/394+21448227/310),4)`$ and the degrees of freedom $\text{df}\approx `r round((61324307/394+21448227/310)^2/((61324307/394)^2/393+(21448227/310)^2/309),4)`$. As a consequence we can compute the $p$-value:
```{r}
2 - 2*pt(4.6795, df=655.3507)
```
and the rejection region from the quantile:
```{r}
qt(0.975, df=655.3507)
```
A more straightforward solution is to use directly the `t.test(.)` function in `R`:
```{r}
t.test(x = data$Price[data$Cylinder==4], y = data$Price[data$Cylinder==6], mu = 0, alternative = "two.sided")
```
It was not obvious from the boxplot that there is a mean difference of price between four and six cylinders. From this hypothesis testing and sample, we can reject $H_0$ at a level of $\alpha=0.1$ in favor of $H_1$, the test is significant.

Similarly a Wilcoxon signed-rank test can be perform if outliers are suspected in the data
```{r}
wilcox.test(x = data$Price[data$Cylinder==4], y = data$Price[data$Cylinder==6], mu = 0, alternative = "two.sided")
```

#### One population, two different situations
In this situation, we want to test the average of a population before and after a change occurs. For example, patients before and after a treatment. In this case, we create one sample by taking the difference of the two samples (before/after). It is possible because the same individuals (or testing units) are p The values we are interested in for the test are the same as the test one a single mean.

**Example 6** We are interested in the effect of a soporific drug on 10 patients. The study wants to show an increase in extra hours of sleep and a risk of 10\% is acceptable to reach this conclusion.
```{r, cache=TRUE, fig.align="center", fig.height=4, fig.width=6}
# load dataset
data(sleep)

# Creates the new dataset
df <- data.frame(
        sample1 = sleep$extra[sleep$group==1],
        sample2=sleep$extra[sleep$group==2],
        new_sample = sleep$extra[sleep$group==1] - sleep$extra[sleep$group==2]
        )
df

# Graph
ggplot(data=sleep, aes(x=group, y=extra, group=group, fill=group)) +
        geom_boxplot()

# Paired t-test
t.test(extra ~ group, mu = 0, alternative = "less", paired = TRUE, data = sleep) 
# "extra ~ group" can be read as "extra by group".
# Here it is equivalent to "x = extra[group==1], y = extra[group==2]"
```

[In-class: exercise 7](https://samorso.github.io/RM2017/Inclass.html#ex7)   
[Homework: exercise 4](https://samorso.github.io/RM2017/hmw.html#ex4)  

### ANOVA
The ANalysis Of VAriance (ANOVA) regroups many tests and models in statistics. In this class I only discuss the **one-way ANOVA** test. The one-way ANOVA test corresponds to the following situation: we have a sample separated into $k$ factors. We want to test whether under the null-hypothesis the sample is drawn from one population, in which case each sub-samples are assumed to have the same mean, against the alternatives that the sub-samples are drawn from more than one population. This test further assumes that each sub-samples are normally distributed and homeostatic (same variances) under $H_0$. 

 Interest          |  Value                                                  
------------------ | -------------------------------------------------
Test statistic     | $T = \frac{\text{MS}_\text{Treatment}}{\text{MS}_{\text{Residuals}}}$, where $\text{MS}$ means ``mean squares''.
$H_0$              | $\mu_1 = \mu_2 = \dots = \mu_k$   (all averages are equal)
$H_1$              | At least one difference
$p$-value          | $\mathbb{P}_{H_0}\big(T>t_{\text{obs}}\big) = 1 - F_{\text{df}_1;\;\text{df}_2}(t_{\text{obs}})$
$T(\alpha)$        | $t_{\text{df}_1;\;\text{df}_2;\;1-\alpha}$
Rejection region   | $[t_{\text{df}_1;\;\text{df}_2;\;1-\alpha},+\infty)$

First, remark this test cannot be oriented (no options on the alternative). The distribution $F_{\text{df}_1;\;\text{df}_2}$ is the $F$ distribution, also known as *Fisher-Snedecor distribution*. It has two parameters, the degrees of freedom. Here $\text{df}_1 = k - 1$ and $\text{df}_2 = n - k$, where $k$ is the number of factors and $n$ the total sample size. The $\text{MS}$ is obtained by dividing the sum of squares by the degrees of freedom. 

In order to compute the sum of squares, we use the following relation: $$ \text{SS}_{\text{Total}} = \text{SS}_{\text{Treatment}} + \text{SS}_{\text{Residuals}}, $$ where $\text{SS}$ stands for **S**um of **S**quares. The idea is that the total variability can be decomposed into the variability **within** each subgroup (Residuals) and the variability **betweem** subgroups (Treatment). It is easy to derive the total sum of squares. Let $\hat{\mu}$ denotes the overall sample average. Then $$ \text{SS}_{\text{Total}} = \sum_{i=1}^n (x_i - \hat{\mu})^2$$ Now we can denote $\hat{\mu}_j$ and $n_j, j=1,\dots,k$ the $k$ subgroup means and sample sizes. The treatment sum of squares is given by $$ \text{SS}_{\text{Treatment}} = \sum_{j=1}^k n_j\cdot(\hat{\mu}_j - \hat{\mu})^2 $$ The residual sum of squares is directly obtained by taking the difference between total and treatment sum of squares.

**Example 5:** Here is a minimal example to construct the one-way ANOVA test. Suppose we want to test under the null-hypothesis that there is no treatment effect (all average equals) at a level of $\alpha=5\%$.

Treatment       | Observations          | Sample size   | Average
--------------- | --------------------- | ------------- | ---------------
A               | $3\;\;\;5\;\;\;7$     | 3             |        5  
B               | $4\;\;\;2$            | 2             |        3  
C               | $1\;\;\;7$            | 2             |        4       
Overall         |                       | 7             | `r (mu <- round(mean(c(3,5,7,4,2,1,7)),4))`         

The total sum of squares is obtained by $$ \text{SS}_{\text{Total}} = (3-4.1419)^2+(5-4.1419)^2+(7-4.1419)^2+(4-4.1419)^2+(2-4.1419)^2+(1-4.1419)^2+(7)^2 \approx `r (ss_tot <- round(sum((c(3,5,7,4,2,1,7)-mu)^2),4))` $$ and the treatment sum of squares by $$ \text{SS}_{\text{Treatment}} = 3(5-4.1429)^2 + 2(3-4.1429)^2 + 2(4-4.1429)^2 \approx `r (ss_tr <- round(3*(5-mu)^2 + 2*(3-mu)^2  + 2*(4-mu)^2,4))`$$ By deduction, we have also the residuals sum of squares given by $$ \text{SS}_{\text{Residuals}} = \text{SS}_{\text{Total}} - \text{SS}_{\text{Treatment}} = `r ss_tot - ss_tr` $$ We can now construct this nice table

Quantity                       | Value
------------------------------ | --------------
$\text{df}_1$                  | $2 (=3-1)$  
$\text{df}_2$                  | $4 (=7-3)$  
$\text{SS}_{\text{Treatment}}$ | `r ss_tr`
$\text{SS}_{\text{Residuals}}$ | `r ss_tot - ss_tr`  
$\text{MS}_{\text{Treatment}}$ | `r (ms_tr <- round(ss_tr / 2,4))` 
$\text{MS}_{\text{Residuals}}$ | `r (ms_rs <- round((ss_tot - ss_tr) / 4,4))`
$t_{\text{obs}}$               | `r (tobs <- round(ms_tr / ms_rs,4))`  

The $p$-value can be obtained using the following line of code
```{r}
1 - pf(tobs, df1 = 2, df2 = 4) # tobs is already set
```
We can also compute $T(\alpha)$
```{r}
(t_alpha <- qf(0.95, df1 = 2, df2 = 4))
```
Hence the rejection region is $[`r round(t_alpha,4)`;\;+\infty)$.

By looking at the $p$-value (or equivalently the rejection region), we can say that there is not enough evidence to conclude the test at a level of 5\%.

**CS:** We want to test if the price for all cars is equivalent regardless the type of the cars for level of significance of $\alpha=5\%$.
```{r,fig.align='center',fid.width=6,fig.height=4,cache=TRUE}
# Multiple boxplots
ggplot(data=data, aes(x=Type, y=Price, group=Type, fill=Type)) +
  geom_boxplot()

# Perform an ANOVA test
anova_test <- aov(Price ~ Type, data = data)
summary(anova_test)
```

From the `R` summary of the `aov(.)` function we can read

Quantity                       | Value
------------------------------ | --------------
$\text{df}_1$                  | $4 (=5-1)$  
$\text{df}_2$                  | $799 (=804-5)$  
$\text{SS}_{\text{Treatment}}$ | $2.409\times10^{10}$  
$\text{SS}_{\text{Residuals}}$ | $5.437\times10^{10}$  
$\text{MS}_{\text{Treatment}}$ | $6.023\times10^{9}$  $(= \text{SS}_{\text{Treatment}} / \text{df}_1)$  
$\text{MS}_{\text{Residuals}}$ | $6.805\times10^{7}$  $(= \text{SS}_{\text{Residuals}} / \text{df}_2)$  
$t_{\text{obs}}$               | $88.51$ $(= \text{MS}_{\text{Treatment}} / \text{MS}_{\text{Residuals}})$  
$p$-value                      | $<2\times10^{-16}$ $(=1-F_{4;\;799}(88.51))$  

What is the conclusion?

If you suspect the presence of outliers (from the graph for example), there is a test that is more robust to outliers, the **Kruskal-Wallis** test. Additionally, it does not assume normality of the subgroups (but it is less powerful). You can use this test by running the `R` code:
```{r}
kruskal.test(Price ~ Type, data = data)
```


### Multiple pairwise comparison
If the one-way ANOVA is conclusive, it can be interesting to know more precisely for which pair we have a difference: we can make multiple pairwise comparison. But one should be careful with the significance level! If you accept a level of $\alpha$ for every single test, the global level you are accepting is not $\alpha$. 

Suppose that you accept globally $\alpha = 5\%$, what should be the individual level of significance $\alpha_I$? You know that for one test the probability to be non-significant is $1-\alpha_I$. Now if you run $c$ pairs of tests, the probability that none of them are significant is $(1-\alpha_I)^c$ if you assume independence between the tests (just take their product). In this context, Sidak proposes that $\alpha$ corresponds to the probability that at least one of the test is significant under the null-hypothesis, i.e. $$ \mathbb{P}_{H_0}(\text{at least one test significant}) = \alpha.$$ Because this probability is 1 minus the probability that no tests are significant, the correction is obtained $$ \alpha = 1 - (1 - \alpha_I)^c \Rightarrow \alpha_I = 1 - (1 - \alpha)^{1/c}. $$
This is the **Dunn-Sidak correction**. It should be mentioned (once again) that $\alpha$ controls the error rate only when all the null-hypothesis are true, i.e. it controls the type I error.

For example if I want to compare $8$ pairs with an $\alpha=5\%$, then the individual level of significance using the Dunn-Sidak correction is $\alpha_I = `r round(1 - .95^(1/8),4)`$ (much smaller)! It means that by comparing each $p$-values with a level of $\alpha_I$, we globally accept a level of $\alpha$.

**CS:** The one-way ANOVA test of the cars price by type was significant. We test pairwise with a global level of significance $\alpha=5\%$.  

There are five types of cars ($k=5$). How many pairs are we comparing? Recall from your math course that we have $$ c = \begin{pmatrix} k \\ 2 \end{pmatrix} $$ pairs. Here $c=10$. The Dunn-Sidak correction is $\alpha_I = 1 - 0.95^{0.1} = `r round(1 - .95^.1,4)`$. To make all the comparison, we can use the `pairwise.t.test(.)` function in `R`: 
```{r}
pairwise.t.test(x = data$Price, g = data$Type, p.adjust.method = "none", pool.sd = FALSE)
```
With the Dunn-Sidak correction, we reject $H_0$ for each pair except _Wagon_ against _Sedan_ types of cars.

If instead of a one-way ANOVA test you used the robust alternative, the Kruskal-Wallis test. It would make more sense to make the pairwise comparisons with a robust test as well, the Wilcoxon's signed-rank test. It can be easily performed in `R` using the `pairwise.wilcox.test(.)` function:
```{r}
pairwise.wilcox.test(x = data$Price, g = data$Type, p.adjust.method = "none")
```
With these tests, we reject $H_0$ for every pairs at level of significance of $\alpha_I = `r round(100 * (1 - .95^.1),2)`\%$. Looking more closely to the boxplots, we see that few outliers appears for the _Sedan_ of cars. If we look at the five-numbers-summary
```{r}
summary(data$Price[data$Type=="Sedan"])
```
we see that outliers have an influence on the average (`mean` $\gg$ `median`) making certainly the $t$-test vulnerable, whereas the Wilcoxon test is less influenced.

[In-class: exercise 8](https://samorso.github.io/RM2017/Inclass.html#ex8)   
[Homework: exercise 5](https://samorso.github.io/RM2017/hmw.html#ex5)  

### Some practical recommendations    

* **How should I orientate the test?** Orienting a test gives you a more precise answer but it should be decided before looking at the data.  
* **What should be my significance level $\alpha$?** If no precision is given, a default value for this class is $5\%$. Never change $\alpha$ after performing a test (although tempting).   
* **Robust or not robust?** First, always check the data with descriptive techniques such as graph to have an intuition. If it is evident there are outliers, perform a robust test (or the converse). If it is not clear, run both robust and non-robust test. The non-robust test is generally more powerful but at the price of being more sensitive.  
* **Can I trust the results?** Statistical test is a useful tool, but it should be used with caution. They are based on assumptions (we discussed some) that are not always met. A descriptive analysis should help you appreciate these assumptions, and therefore the validity of the test.  

# Multivariate Regression model
## Model construction
In the Introduction we formalized the regression equation: $$ y_i = \beta_0 + \sum_{j=1}^px_{ij}\beta_j + \varepsilon_i,\quad i = 1,\dots,n. $$ It is not unusual to write it even more compactly in matrix form: $$ \mathbf{y} = \mathbf{X}\boldsymbol\beta  + \boldsymbol\varepsilon $$ We have $\mathbf{y}$ is a vector of size $n\times 1$, $\mathbf{X}$ is a matrix of dimensions $n\times(p+1)$, $\boldsymbol\beta$ is a vector of size $(p+1)\times 1$ and $\boldsymbol\varepsilon$ is a vector $n\times 1$. The matrix of **predictors** $\mathbf{X}$ is called the **design matrix**. Predictors can be split into two categories:    

1. **Covariates**: they are numerical quantities (for example _Mileage_ in the **CS**);   
2. **Factors**: they take only few values, called _levels_ (for example _Cylinder_ in the **CS**). Factors can quantitative or qualitative, ordinal or not.      

When you have factors in the model, they are re coded into specific numerical values, the **contrast**. The contrast allows comparison between the different levels of a factor.

**CS:** For example you want to regress the _Price_ of a car with the ordinal factor _Cylinder_. In `R` you can fit the regression model easily. First you need to make sure `R` knows that _Cylinder_ is a factor.
```{r}
# Check the structure of the data
str(data)

# Tell R that Cylinder is a factor (it modifies the data)
data$Cylinder <- as.factor(data$Cylinder)

# Fit linear regression
fit_lm <- lm(Price ~ Cylinder, data)
summary(fit_lm)
```
You can see that there are two coefficients. Why not three? There are three levels for _Cylinder_. In fact the level _Cylinder4_ is included in the intercept by default, we say that _Cylinder4_ is the reference. The regression coefficients represent the average difference between _Cylinder6_ (or _Cylinder8_) and _Cylinder4_. The design matrix is of dimension $804\times 3$. If I take one row of the design matrix $\mathbf{X}$, the first column is 1 for the intercept and _Cylinder4_, the second column takes dummy values 1 if the car has _Cylinder6_ or 0 elsewhere, same for third column with _Cylinder8_. The regression line is $$ y_i = \beta_0 + \beta_1\text{Cylinder}_6 + \beta_2\text{Cylinder}_8 + \varepsilon_i,\quad i=1,\dots,n.  $$ The variables $\text{Cylinder}_6$ and $\text{Cylinder}_8$ takes dummy values 0 or 1. The estimated regression line is $$ \hat{y}_i = `r round(coef(fit_lm)[1],2)` + `r round(coef(fit_lm)[2],2)`\text{Cylinder}_6 + `r round(coef(fit_lm)[3],2)`\text{Cylinder}_8, \quad i=1,\dots,804.$$

So if I have a new car with four cylinder, the predicted price is $$\hat{y}^{\text{new}} = `r round(coef(fit_lm)[1],1)`$$ With six cylinders it is $$\hat{y}^{\text{new}} = `r round(coef(fit_lm)[1],1)` + `r round(coef(fit_lm)[2],1)` \approx `r round(coef(fit_lm)[1]+coef(fit_lm)[2],1)`$$ And with eight $$\hat{y}^{\text{new}} = `r round(coef(fit_lm)[1],1)` + `r round(coef(fit_lm)[3],1)` \approx `r round(coef(fit_lm)[1]+coef(fit_lm)[3],1)`$$ Note that the contrast I presented is called _treatment_ (by default in `R`) and that other exist.

If you want to see the details of the design matrix, you can run the commands:
```{r}
# Extract the design matrix
X <- model.matrix(fit_lm)
head(as.data.frame(X))

# Compare with the data
head(data[,c(1,7)])
```

Note that if you prefer to have another level of cylinder as reference you can follow the code:
```{r}
# The levels of Cylinder
levels(data$Cylinder)

# Modify the reference level for Cylinder
data <- within(data, Cylinder <- relevel(Cylinder, ref = 2))

# Check the difference
levels(data$Cylinder)

# Fit the new regression with Cylinder6 in the intercept
fit_lm <- lm(Price ~ Cylinder, data)
summary(fit_lm)
```

[In-class: exercise 9](https://samorso.github.io/RM2017/Inclass.html#ex9)   

## Model checking
Once the model is fitted, it is necessary to check the validity of the fit. Model checking is perform on the residuals, i.e. the difference between the response and the fitted values $\hat{\varepsilon}_i = y_i - \hat{y}_i$. It is common to do the checking on the standard residuals, $r_i = \hat{\varepsilon}_i/\hat{\sigma}_{\varepsilon}$. There are many sources of difference between the data and the assumed regression model. The difference can be specific or systematic. If specific, it means that very few data points behaves differently from the major part, we talk about _outliers_ in this case. Exploratory analysis usually helps to identify such data points. A robust linear model may help to decrease the influence of outliers. The handling of systematic difference will depend on the underlying assumption affected:   

* _normality_ : residuals should behave approximately as normally distributed. Non-normality may be more often a consequence of other problems in the model. It is used to assess the general good fit of the model.    
* _constant variance_ or _homoscedasticity_ : the variance is the same across the response variable, it does not increase or decrease with the level of the response. If this problem appears, sometimes a data-transformation can solve this problem.     
* _linearity_ : the response depends linearly on each covariates.    
* _independence of predictors_ : predictors are supposed to be not too strongly linearly dependent. If they are too strongly correlated, there is a problem of multicollinearity.   
* _independence of the response_ : there should be no serial correlation on the residuals.   

I discuss each problem separately, but keep in mind that in practice they may occur jointly.

### Normality
Normality is checked with a _quantile-quantile plot_ or _QQplot_. The observed quantiles on the data are plotted against the theoretical quantiles of a normal random variable. If normality is doubtful, it is a alarm telling you that there might be a problem with the other assumptions mentioned.

**Example 1:** Let see the qqplot of the regression of the distance explained by the speed of the cars.
```{r,fig.align='center',cache=TRUE,message=FALSE}
# the regression
fit_lm <- lm(dist ~ speed, data = cars)

# qq-plot
require(ggfortify)
autoplot(fit_lm, which = 2, label.size=3)
```

Theoretically (or if sample size is very large), the observed quantiles should match the theoretical one and appear on the line. Here, since the sample size is small, it is not unusual to have this pattern and it is hard to tell there is a problem of normality. Other check might be helpful.

[In-class: exercise 10](https://samorso.github.io/RM2017/Inclass.html#ex10)   

### Homoscedasticity
The problem of non-constant variability is checked by ordering the standard residuals with respect to the fitted value, $\hat{y}_i$. Roughly speaking, the points should appear randomly distributed around the center and no specific pattern should appear on scatterplot.

**Example 1:** Back to this example.
```{r,fig.align='center',cache=TRUE,message=FALSE}
autoplot(fit_lm, which = 1, label.size = 3)
```

Here it is hard to argue that there is a specific pattern and the graph seems ok. If there is a problem, it is often linked with a problem of non-linearity (see next subsection).

**Example 1:** Now I modify the dataset to illustrate a problem of homoscedasticity.
```{r,message=F,warning=F,fig.align='center',cache=T}
# copy cars dataset
cars2 <- cars

# extract coefficients from the regression
beta <- coef(fit_lm)

# define new variance and errors
sigma2 <- cars2$speed^3
set.seed(4322)
eps <- rnorm(nrow(cars2), sd=sqrt(sigma2))

# simulate new dist variable
cars2$dist <- beta[1] + beta[2] * cars$speed + abs(eps) 
# Note that I use the absolute value of the error to avoid negative distance

# fit the regression
fit_lm2 <- lm(dist ~ speed, data = cars2)

# plot
require(ggfortify)
autoplot(fit_lm2, which = 1, label.size = 3)
```
Now it is more obvious that the variance is non-constant, it increases with the value of speed. To fix this problem, a possibility is to transform the data with a logarithm. In this example, I transform the response variable with a logarithm (see the code below), but sometimes the predictor can be transformed or both.

```{r,fig.align='center',cache=T,message=F,warning=F}
# log-transform the dist variable and create a new dataset
cars3 <- within(cars2, dist <- log(dist))

# fit the new regression
fit_lm3 <- lm(dist ~ speed, data = cars3)

# plot the new fit
autoplot(fit_lm3, which = 1, label.size = 3)
```

[In-class: exercise 11](https://samorso.github.io/RM2017/Inclass.html#ex11)  

### Linearity
Linearity between the response variable and predictors is checked on the explanatory analysis, but it can also be checked by plotting the residuals against each predictor, in which case no specific pattern should appear, the residuals and predictor should ''look independent''.

If I simulate two independent random variables (approximately the kind of shape you should observe), I should observe a 'cloud of dots'.
```{r,fig.align='center',cache=T}
set.seed(321)
d_f <- data.frame(
  x1 = rnorm(1000),
  x2 = rnorm(1000)
)
ggplot(d_f, aes(x=x1,y=x2)) + geom_point()
```

**Example 1:** With few data points, it is hard to tell whether there is a pattern or not. Here the linearity assumption seems to hold.
```{r,fig.align='center',cache=T}
d_f <- data.frame(
  residual = residuals(fit_lm),
  speed = cars$speed
)
ggplot(d_f, aes(x=speed,y=residual)) + geom_point()
```

**Example 4:** Let see what happens when the relationship between the response and the predictor is curvilinear.
```{r,fig.align='center',cache=T}
# Generate random values
set.seed(1234)
x <- rnorm(1000)
y <- x * x + rnorm(1000)

# fit a regression model
fit_lm <- lm(y ~ x)

# plot residuals agains the covariate x
d_f <- data.frame(
  residual = residuals(fit_lm),
  x = x
)
ggplot(d_f, aes(x=x,y=residual)) + geom_point()
```

Clearly a pattern appears (remember the plot in the Exploratory Analysis chapter?). In order to take this relationship into account in the regression model, we can add a 'new' variable, the square term (using the `I(.)` function in `R`).

```{r,fig.align='center',cache=T}
# fit a regression model
fit_lm2 <- lm(y ~ x + I(x^2))

# plot residuals agains the covariate x
d_f <- data.frame(
  residual = residuals(fit_lm2),
  x = x
)
ggplot(d_f, aes(x=x,y=residual)) + geom_point()
```

The difference is striking. If we look at the `summary`, we can see that the square term is clearly significant.
```{r}
summary(fit_lm2)
```

More generally, we can add to the regression model any polynomial, and the task is easy with `R` as you can simply use the function `poly(x,p)` where `x` is the variable and `p` is the degrees of the polynomial. To fix ideas, let revisit the previous example, modeling this time a polynomial of degree three. 

```{r,fig.align='center',cache=T}
# fit a regression model
fit_lm3 <- lm(y ~ poly(x,3))

# plot residuals agains the covariate x
d_f <- data.frame(
  residual = residuals(fit_lm3),
  x = x
)
ggplot(d_f, aes(x=x,y=residual)) + geom_point()
```

Note that using `poly(x,3)` is equivalent as writting `x + I(x^2) + I(x^3)`. The regression model is the following: $$ y_i = \beta_0 + \beta_1\;x_i + \beta_2\;x_i^2 + \beta_3\;x_i^3 + \varepsilon_i, \quad i=1,\dots,n. $$

Let's have a look at the summary.
```{r}
summary(fit_lm3)
```

Not surprisingly, the third term of the polynomial is not significant (remember we use only the second term when simulating the data). How would you write the estimated regression?

[In-class: exercise 12](https://samorso.github.io/RM2017/Inclass.html#ex12)  

### Independence of predictors
The predictors should be not too strongly linearly correlated. The intuition is that if two predictors have a strong linear relationship, they carry the same amount of information for the regression model, and thus only one of them is necessary. The presence of correlated predictors in the model have a undesirable effects: the estimators are numerically unstable, you cannot rely on the numerical value obtained.  

This problem is refered as the mulitcollinearity problem. One possibility to spot a problem of multicollinearity is with a scatterplot between predictors, so with the exploratory analysis tools. This problem can be considered as minor because nowadays statistical software deal quite well with these numerical issues, and if a model selection is performed (next section), the extra predictors will, in general, not be selected

We now illustrate a problem of multicollinearity by using the same simulation as in **Example 4** :
```{r,fig.align='center',cache=T}
# Generate random values
set.seed(1234)
x <- rnorm(1000)
y <- x + rnorm(1000)

# Create a new variable very similar to "x"
x2 <- x + rnorm(1000,sd=.1)

# Scatterplot of x and x2
require(ggplot2)
qplot(x,x2)

```
(Here the linear correlation is `r cor(x,x2)`).

```{r}
# fit a regression model
fit_lm <- lm(y ~ x)
fit_lm1 <- lm(y ~ x2)
fit_lm2 <- lm(y ~ x + x2)
```

See the effect of multicollinearity on the regression coefficients
```{r,echo=F}
d_f <- data.frame(
  model = c("fit_lm", "fit_lm1", "fit_lm2"),
  intercept = c(coef(fit_lm)[1], coef(fit_lm1)[1], coef(fit_lm2)[1]),
  beta_1 = c(coef(fit_lm)[2], NA, coef(fit_lm2)[2]),
  beta_2 = c(NA, coef(fit_lm1)[2], coef(fit_lm2)[3])
)
d_f
```

But nothing particular appears on the residuals vs predictors plots, they seem to be independent, an indicator for a good fit. The descriptive analysis is thus essential here to early detect a problem of multicollinearity, but as said previously, performing  model selection will generaly help to eliminate such situation.
```{r,fig.align="center",cache=T} 
# plot residuals agains the covariates x and x2
d_f <- data.frame(
  residual = residuals(fit_lm2),
  x = x,
  x2 = x2
)
require(ggplot2)
require(gridExtra)
p1 <- ggplot(d_f, aes(x=x,y=residual)) + geom_point()
p2 <- ggplot(d_f, aes(x=x2,y=residual)) + geom_point()
grid.arrange(p1,p2,nrow=1)
```

[In-class: exercise 13](https://samorso.github.io/RM2017/Inclass.html#ex13)  

### Independence of the response
Each realizations of the response variable given the predictors are assumed to be independent from each other by the model. Saying it with other words, the response variable is assumed to be identically and independently distributed (according to which probabilistic distribution?). It means for example that $y$ is not supposed to explain itself, situation that could appear for example in the context of time series, otherwise it should be taken into account in the regression model. First, we will use one specific tool to spot such situation: the autocorrelation plot. This plot visually shows the correlation between the same variable but at some increasing lag distance (**autocorrelation**).

**Example 6:** Suppose we receive some observations from the following assumed model: $$y_i = \beta_0 + \beta_1\;x_i + \alpha\;y_{i-1} + \varepsilon_i,\quad i = 1,\dots,n.$$. 
```{r}
# sample size
n <- 100

# coefficients
beta0 <- 1
beta1 <- 2
alpha <- 0.5

# Generate observations
y <- numeric(n)
set.seed(43)
eps <- rnorm(n)
x <- rnorm(n)
y[1] <- beta0 + beta1 * x[1] + eps[1]
for(i in 2:n){
  y[i] <- beta0 + beta1 * x[i] + alpha * y[i-1] + eps[i]
}
```

To plot the autocorrelation we use the `forecast` package.
```{r,eval=F}
install.packages("forecast")
require(forecast)
```

```{r,echo=F,warning=F,message=F}
require(forecast)
```

```{r,cache=T,fig.align='center'}
ggAcf(y)
```

Each *bar* represents the correlation between $y_i$ and $y_{i+l}$ where $l$ is the lag. We can clearly identify that the first lag ($l=1$), the autocorrelation is about $0.5$ as construncted with this model. For the scope of this course, we do not cover the modelisation of lagged variable but you should be aware of this phenomena.   

[In-class: exercise 14](https://samorso.github.io/RM2017/Inclass.html#ex14)  

### Robust regression
The problem of outlier is specific: only few data points are concerned. To handle this problem, we can fit a robust regression. Let's illustrate with **Example 5**:

Recall that we had the following in the explanatory analysis:
```{r,fig.align='center',fig.height=4,fig.width=6,message=FALSE,cache=TRUE}
# Generate random values
set.seed(1234)
x <- rnorm(1000)
y <- x + rnorm(1000)

# We add randomly some outliers
y[sample(1000,10)] <- rnorm(10,mean=15)

# Plot  
qplot(x,y)
```

Now suppose you want to model the response $y$ with the covariate $x$ (intercept $\beta_0$ is 0 and coefficient $\beta_1$ is 1). 
```{r,cache=T}
# Linear model
fit_lm <- lm(y ~ x)
summary(fit_lm)

# Robust linear model from MASS package
require(MASS)
fit_rlm <- rlm(y ~ x)
summary(fit_rlm)
```

You can see that the intercept is affected (significantly different from 0 with the linear model).

On a practical note, you can always try both robust and regular and see whether there are differences. If there are none, keep the regular, if there are, use your intuition from the explanatory variable to decide which approach is the best. 

[In-class: exercise 15](https://samorso.github.io/RM2017/Inclass.html#ex15)  

## Model selection
The goal of model selection is to provide us with the model the generalize the most, in the sense that it explains well the population (and not only the sample). The goal is to choose the right combination of predictors as some might be irrelevant (remember for example the problems of multicollinearity and/or non-linearity). For this purpose, the AIC (*Akaike Information Criterion*) is a popular option: it gives a score to each possible models. You achieve model selection by taking the model that minimises the scores.

**Example 7:** We use the `mtcars` dataset for which we want to explain the miles per gallon with respect to different predictors (10).
```{r,cache=T}
data(mtcars)
head(mtcars)
str(mtcars)

# Modify into factors
mtcars2 <- mtcars
mtcars2$cyl <- as.factor(mtcars$cyl)
mtcars2$vs <- as.factor(mtcars$vs)
mtcars2$am <- as.factor(mtcars$am)
mtcars2$gear <- as.factor(mtcars$gear)
mtcars2$carb <- as.factor(mtcars$carb)
str(mtcars2)
```

We created a copy of the dataset where we modify some predictor to be factors. For the sake of the exercise, we fit a regression model where for every continuous predictor we add the polynomial of order 3 ($p=?$). We now select the best model according to the AIC:
```{r,cache=T}
fit_lm <- lm(mpg ~ cyl + poly(disp,3) + poly(hp,3) + poly(drat,3) + poly(wt,3) + 
               poly(qsec,3) + vs + am + gear + carb, data = mtcars2)
require(MASS)
stepAIC(fit_lm)
```

Here the function `stepAIC` consider `poly(predictor,3)` as 'one predictor' in the sense either all the three polynomes are included or all are excluded. To refine the result, we can do the following instead:

```{r,cache=T}
fit_lm2 <- lm(mpg ~ . + I(disp^2) + I(disp^3) + I(hp^2) + I(hp^3) + 
                I(drat^2) + I(drat^3) + I(wt^2) + I(wt^3) + I(qsec^2) + I(qsec^3), data = mtcars2)
fit_aic <- stepAIC(fit_lm2)
```

The initial model has a AIC score of `r extractAIC(fit_lm2)[2]`. The final (with minimum score found) model has an AIC score of 24.57. It is the following: $\begin{align*} \text{mpg} &= \beta_0 + \beta_1\;\text{cyl}_6 + \beta_2\;\text{cyl}_8 + \beta_3\;\text{disp} + \beta_4\;\text{drat} + \beta_5\;\text{wt} + \beta_6\;\text{qsec} + \beta_7\;\text{vs}_1 + \beta_8\;\text{gear}_4 + \beta_9\;\text{gear}_5 + \\ &\quad\beta_{10}\;\text{carb}_2 + \beta_{11}\;\text{carb}_3 + \beta_{12}\;\text{carb}_4 + \beta_{13}\;\text{carb}_6 + \beta_{14}\;\text{carb}_8 + \beta_{15}\;\text{disp}^2 + \beta_{16}\;\text{disp}^3 + \beta_{17}\;\text{hp}^3 \\ &\quad + \beta_{18}\;\text{drat}^2 + \beta_{19}\;\text{drat}^3 + \beta_{20}\;\text{qsec}^2 + \beta_{21}\;\text{qsec}^3 + \varepsilon \end{align*}$

Can you write the fitted model?

[In-class: exercise 16](https://samorso.github.io/RM2017/Inclass.html#ex16)  

## Prediction
Suppose we receive new data for the predictors (assumely for all of them), we want to be able to say something about the new response that we have not observed yet, we want to predict it. We have already seen how to predict a point. For the above example, suppose we observe a new value for every predictors $x^{\text{new}}$, we can predict the future value for the response $y^{\text{new}}$:

```{r}
x_new <- data.frame(cyl = '6', disp = 150, hp = 102, drat = 2.05, wt = 4.03, 
                    qsec = 15.01, vs = '0', am = '1', gear = '3', carb = '2')
predict(fit_aic, newdata = x_new)
```
Note how we input data for the factors. It is even possible to give this prediction within an interval: a **prediction interval**:
```{r}
predict(fit_aic, newdata = x_new, interval = "predict", level = 0.95)
predict(fit_aic, newdata = x_new, interval = "confidence", level = 0.95)
```

The confidence interval at a $1-alpha$ level for a future observation is defined as $$ \hat{y}^{\text{new}} \mp t_{1-\alpha/2;\;n-p-1}\hat{\sigma}_y$$
The prediction interval is similar, it is given by $$ \hat{y}^{\text{new}} \mp t_{1-\alpha/2;\;n-p-1}\tilde{\sigma}_y,$$ where $\tilde{\sigma}_y>\hat{\sigma}_y$. The intuition is that when we construct a prediction interval we take into account the uncertainty around our estimated coefficients ($\hat{\beta}$) and the gaussian noise ($\varepsilon$) as the assumed model is $y=X\beta+\varepsilon$. When constructing a confidence interval, the noise is not taken into account, only the uncertainty around the coefficients, thus we have necessarily $\tilde{\sigma}_y>\hat{\sigma}_y$, meaning the prediction interval will be always larger than confidence interval for the same level.

[In-class: exercise 17](https://samorso.github.io/RM2017/Inclass.html#ex17)  
[Homework: exercise 6](https://samorso.github.io/RM2017/hmw.html#ex6)

# References   

* Reboulleau, J. and Boldi, M.-O. (2016) ''Multivariate statistics''.
* Kuiper, S. (2008) ''Introduction to Multiple Regression: How Much Is Your Car Worth?'' *Journal of Statistics Education* Volume 16, Number 3.   
* Davison, A. (2003) ''Statistical Models''  
* Kuffner, T. and Walker, S. (2017) ''Why are p-values controversial?'' *The American Statistician* (just-accepted) http://dx.doi.org/10.1080/00031305.2016.1277161   